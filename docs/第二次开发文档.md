## **知识智能平台 - 第二次开发文档 (v2.0)**

### 1. 核心目标回顾

本次开发的核心目标是根据《后端架构设计书》 设定的蓝图，将项目从原型阶段（一个 `main.py` 文件）**彻底重构为分层式架构**，并实现前端 `Pinia store` 文件所需的所有 API 端点。

### 2. 架构落地与重构

我们成功地实现了《后端架构设计书》中定义的每一个层级：

* **`/app/main.py`**: 已被重构为纯粹的应用入口点，仅负责加载 `lifespan` 事件和 API 路由。
* **`/app/core/`**:
    * `config.py`: 成功创建，用于从 `.env` 文件安全加载数据库和 Qdrant 配置。
    * `lifespan.py`: 成功创建，接管了原 `main.py` 中的数据库连接逻辑。它现在负责在应用启动时连接 Qdrant 和初始化 PostgreSQL 数据库表。
* **`/app/db/`**: `session.py` 已创建，负责管理 SQLAlchemy `engine` 和 `SessionLocal`。
* **`/app/api/`**: `router.py` 成功聚合了所有端点，并统一添加了 `/api/v1` 前缀，以匹配 `modelStore_real.js` 和 `knowlegeBase_real.js` 的 API 根路径。
* **`models/`, `schemas/`, `crud/`, `services/`**: 已为 `Models` 和 `KnowledgeBases` 两个实体创建了对应的模块，严格遵守了关注点分离原则。

### 3. 功能实现：双数据库与 API

本次开发最大的成就是实现了完整的双数据库后端系统和配套 API。

#### 3.1 双数据库系统

我们明确了两个数据库的职责，Qdrant **不能**代替 PostgreSQL：

1.  **PostgreSQL (关系型数据库)**:
    * **职责**: 作为系统的“指挥中心”，存储所有**元数据**和**状态**。
    * **实现**: 已添加到 `docker-compose.yml`。`models/` 目录中的 `Model` 和 `KnowledgeBase` 表结构定义了其 schema。
    * **用途**: 存储模型列表、知识库列表及其状态（如 `processing`, `ready`）。

2.  **Qdrant (向量数据库)**:
    * **职责**: 作为知识的“书架”，存储文档和代码块的**向量表示**。
    * **实现**: 由 `services/kb_service.py` 在 `startParsing` 时动态管理。
    * **用途**: 在 `kb_service` 中，我们实现了为每个知识库（例如 `id=3`）创建一个唯一的 Qdrant 集合（例如 `kb_3`）的逻辑。

#### 3.2 API 实现：Models (模型管理)

我们实现了 `modelStore_real.js` 所需的全部 CRUD API。

* **`GET /api/v1/models`**: (fetchModels) 获取所有模型列表。
* **`POST /api/v1/models`**: (addModel) 添加一个新模型。
* **`PUT /api/v1/models/{id}`**: (updateModel) 更新一个模型。
* **`DELETE /api/v1/models/{id}`**: (deleteModel) 删除一个模型。
* **关键依赖**: 我们还在 `Model` 实体中添加了 `dimensions` 字段，这是创建 Qdrant 集合所必需的前置条件。

#### 3.3 API 实现：KnowledgeBases (知识库管理)

这是本次开发最核心的部分。我们实现了 `knowlegeBase_real.js` 中定义的所有复杂 API 和业务逻辑。

* **`GET /api/v1/knowledgebases`**: (fetchKnowledgeBases) 获取所有知识库列表。
* **`GET /api/v1/knowledgebases/{id}`**: (`_pollParsingStatus`) 获取单个知识库的状态，用于前端轮询。
* **`POST /api/v1/knowledgebases`**: (createKnowledgeBase) 创建一个新的知识库条目（仅元数据）。
* **`DELETE /api/v1/knowledgebases/{id}`**: (deleteKnowledgeBase) 删除知识库。
    * **核心逻辑**: `kb_service` 会同时删除 PostgreSQL 中的记录**和** Qdrant 中对应的集合（如 `kb_{id}`）。
* **`POST /api/v1/knowledgebases/{id}/parse`**: (startParsing) **[核心功能]**
    * **同步操作**: 接收 `embedding_model_id`，从 `Models` 表查出其 `dimensions`，立即在 Qdrant 中创建或重建 `kb_{id}` 集合，并将 PostgreSQL 中的状态设置为 `processing`。
    * **异步操作**: 使用 FastAPI 的 `BackgroundTasks` 将耗时的解析任务（`_run_parsing_pipeline`）放入后台执行。
    * **前端交互**: API 立即返回 `processing` 状态，`knowlegeBase_real.js` 会自动开始轮询 `GET /{id}` 接口。
* **`POST /api/v1/knowledgebases/{id}/cancel`**: (cancelParsing) 停止解析。
    * **当前实现**: 已实现 API 端点。它会更新数据库状态以停止前端轮询。

### 4. 关键问题排查与解决

在实现过程中，我们解决了几个关键的启动和运行时错误：

1.  **PostgreSQL 驱动问题 (`ModuleNotFoundError: psycopg`)**:
    * **问题**: `sqlalchemy` 默认的 `postgresql+psycopg` 驱动未安装。
    * **解决**: 我们在 WSL (Linux) 上通过 `sudo apt install libpq-dev` 解决了 C 语言编译依赖，并成功安装了 `psycopg[c]` 驱动。
2.  **Qdrant 客户端版本问题 (`AttributeError: 'QdrantClient' object has no attribute 'health_check'`)**:
    * **问题**: `qdrant-client` 库的 API 方法名与我们代码不匹配，`health_check` 和 `cluster_status` 均不存在。
    * **解决**: 我们诊断出这是库版本不匹配的问题。最终，我们统一改用了一个在所有版本中都存在的、更可靠的方法 `get_collections()` 作为我们的连接检查方式，并同时更新了 `core/lifespan.py`（启动检查）和 `api/endpoints/health.py`（健康检查端点）。

### 5. 当前状态与下一步

**状态**: **后端 API 已准备就绪**。

我们已经拥有一个功能完备、架构清晰的后端服务。它完全满足了 `modelStore_real.js` 和 `knowlegeBase_real.js` 文件的所有 API 需求。

**下一步**:
当前的 `_run_parsing_pipeline` 只是一个模拟进度的**占位符 (STUB)**。
下一步的核心工作是实现**真正的知识摄取管道**，即《企划书》 中定义的：
1.  实现文件上传（当前 API 还不支持）。
2.  加载并解析上传的文件（如代码、Markdown）。
3.  对文件进行智能切分（Code Chunking）。
4.  调用 `embedding_model_id` 对应的模型将切片向量化。
5.  将向量批量存入 Qdrant 中对应的 `kb_{id}` 集合。
6.  在 `_run_parsing_pipeline` 完成后，将状态设置为 `ready`。