好的，我们先将您的项目企划书转换为结构清晰的 Markdown 格式，然后立即进入项目的下一个开发阶段。

-----

# 项目企划书：基于混合RAG架构的知识智能平台

## I. 执行摘要与项目战略目标

[cite\_start]本企划书旨在构建一个高性能、可扩展的知识库管理系统（KBMS），该系统专注于通过配置API将软件项目上传为可管理知识库，并利用先进的检索增强生成（RAG）技术实现 **增强型提示词生成** 和 **结构化知识自动生成（子知识库）** 两大核心功能 [cite: 3][cite\_start]。该平台致力于将复杂的非结构化代码和项目文件转化为易于检索、可供分析的结构化知识资产，从而大幅提升开发人员获取项目核心信息、理解结构关系和进行高效问答的效率 [cite: 3]。

### 1.1 系统核心能力和技术要求回顾

[cite\_start]项目的核心价值在于解决传统文档检索在面对代码结构和深层逻辑时效率低下的问题 [cite: 5][cite\_start]。为此，系统设计必须满足以下关键能力要求 [cite: 5]：

  - [cite\_start]**RAG 增强搜索**：平台需根据用户的输入提示词，执行搜索增强，并最终返回一个结构清晰、携带相关有效信息（例如相关的代码段）的增强提示词 [cite: 6][cite\_start]。这要求后端具备精细的代码分块策略和高精度向量检索能力 [cite: 6]。
  - [cite\_start]**LLM 驱动的结构化知识生成**：系统必须能够调用指定的大模型来分析上传的项目，并为该项目生成描述其数据结构或结构关系的文档作为子知识库 [cite: 7]。
  - [cite\_start]**灵活的知识管理**：用户需要能够上传、下载子知识库，并在生成提示词时灵活选择是否携带相关的子知识库一并进行检索 [cite: 8][cite\_start]。这种选择性检索要求架构支持复杂的元数据管理和多源检索（Agentic RAG） [cite: 8]。
  - [cite\_start]**技术栈要求**：后端必须使用 Python，并依赖开源库来简化RAG和知识库构建过程 [cite: 9][cite\_start]。前端设计则需采用低代码方式实现 [cite: 9]。

## II. 核心系统架构与技术栈选型

### 2.1 后端框架选型：FastAPI的战略优势

[cite\_start]本系统作为知识智能服务平台，其核心任务涉及大量的I/O密集型操作，例如处理外部大型语言模型（LLM）的调用、进行复杂的向量数据库检索以及并发处理多用户请求 [cite: 12][cite\_start]。因此，系统的性能和原生异步处理能力是后端框架选择的首要标准 [cite: 12]。

  - [cite\_start]**推荐技术栈**：Python 3.10+, FastAPI, Uvicorn [cite: 13]
  - [cite\_start]**选择理由**：FastAPI 基于 ASGI 架构，原生支持异步操作，在处理等待 LLM 响应或数据库查询等高延迟I/O任务时不会阻塞服务，能有效处理并发请求 [cite: 14][cite\_start]。相比之下，Django 对于纯 API 服务功能过于冗余，异步支持也较弱 [cite: 15][cite\_start]。FastAPI 还能自动生成 OpenAPI 文档，极大地方便与低代码前端集成 [cite: 15][cite\_start]。持久化层推荐结合使用 SQLAlchemy，它被誉为 Python ORM 的“黄金标准”，并良好支持异步操作 [cite: 15]。

#### Backend Framework Comparison for Generative AI Services

| Feature | FastAPI (Recommended) | Django (Alternative) | Rationale for Selection |
| :--- | :--- | :--- | :--- |
| **Async Support** | [cite\_start]原生支持，基于ASGI [cite: 18] | [cite\_start]需额外配置Django Channels，复杂 [cite: 18] | [cite\_start]确保LLM和RAG调用的高并发和低延迟 [cite: 18]。 |
| **性能表现** | [cite\_start]极高，适合I/O密集型任务 [cite: 18] | [cite\_start]相对开销大，可能需要更多资源 [cite: 18] | [cite\_start]直接满足高性能API服务的隐性要求 [cite: 18]。 |
| **项目契合度** | [cite\_start]专用于API服务，微服务架构理想 [cite: 18] | [cite\_start]更适合大型单体应用或需要全套管理功能 [cite: 18] | [cite\_start]避免功能冗余，保持架构精简 [cite: 18]。 |
| **ORM推荐** | [cite\_start]SQLAlchemy (被誉为Python ORM的黄金标准) [cite: 18] | Django ORM (Opinionated) | [cite\_start]灵活且稳定，支持异步操作 [cite: 18]。 |

### 2.2 知识库管理系统（KBMS）技术选型

#### 2.2.1 向量数据库：Qdrant

[cite\_start]Qdrant 被选为核心向量数据库，负责存储和检索所有知识库的向量表示 [cite: 21][cite\_start]。它提供完善的 Python 客户端 `qdrant-client`，支持通过 Docker 快速部署，是 RAG 管道的标准组件 [cite: 21]。

#### 2.2.2 RAG 与 LLM 编排层：LlamaIndex/LangChain

[cite\_start]为了构建复杂的 RAG 管道和管理多源检索（Agentic RAG），将采用 LlamaIndex 或 LangChain 作为核心编排框架 [cite: 23, 6][cite\_start]。这些框架负责文档加载、切分、向量化、检索链管理以及与 LLM 的通信 [cite: 23]。

#### 2.2.3 嵌入模型：FastEmbed/BAAI/bge-small-en-v1.5

[cite\_start]为保证检索精度，将采用高质量的开源嵌入模型，例如 Qdrant 官方推荐的 `BAAI/bge-small-en-v1.5`，可通过 FastEmbed 库高效实现向量化 [cite: 25, 9]。

### 2.3 元数据持久化策略

[cite\_start]系统需要一个关系型数据库来存储配置信息和知识库结构，以实现“被管理的知识库”这一核心功能 [cite: 27]。

  - [cite\_start]**技术栈**：PostgreSQL 或 SQLite (用于开发/测试)，结合 SQLAlchemy ORM [cite: 28]。
  - [cite\_start]**作用**：存储系统配置、KB 注册表（主知识库信息）和子知识库清单（Sub-KB 的详细信息，包括类型、状态等） [cite: 29, 30, 31]。

## III. 知识库架构与多层RAG实现深度解析

### 3.1 知识库的定义和分层 Schema

[cite\_start]知识库被划分为四个逻辑层级，通过元数据进行统一管理 [cite: 35]：

  - [cite\_start]**L1：主知识库 (Primary KB)**：存储用户上传的原始代码和项目文档 [cite: 36]。
  - [cite\_start]**L2a：生成式子知识库 (Vectorized Sub-KB)**：存储 LLM 分析项目后生成的文本描述文档 [cite: 37]。
  - [cite\_start]**L2b：生成式子知识库 (Structural KG)**：存储 LLM 提取的关系三元组（知识图谱） [cite: 38]。
  - [cite\_start]**L3：上传式子知识库 (User Uploaded)**：存储用户手动上传的特定文档 [cite: 39]。

### 3.2 深度洞察：混合RAG与知识图谱（KG）的集成

[cite\_start]为了精确回答“哪个类继承自类 A？”这类明确的实体关系查询，系统引入了**知识图谱（Knowledge Graph, KG）** 作为 L2b 子知识库 [cite: 42, 41][cite\_start]。LLM 在分析代码后会执行“三元组提取”任务，将代码关系转化为“主体-谓词-客体”的三元组 [cite: 42][cite\_start]。这种混合 RAG 架构（向量检索 + 结构化 KG 检索）能有效满足用户对项目结构分析的深度需求 [cite: 43]。

### 3.3 Ingestion Pipeline：代码切分与向量化

[cite\_start]项目上传后，数据将进入知识摄取管道 [cite: 45]：

1.  [cite\_start]**文档加载与解析**：使用 LlamaIndex/LangChain 的专业加载器处理项目文件 [cite: 46]。
2.  [cite\_start]**代码智能切分（Code Chunking）**：基于代码的语义结构（如函数、类定义）进行切分，确保每个向量块包含一个功能清晰的代码片段 [cite: 47]。
3.  [cite\_start]**向量化与元数据存储**：生成向量并与原始文本、文件名、代码行号等关键元数据一同存入 Qdrant [cite: 48, 9]。

## IV. 双循环 RAG 流程：Prompt增强与知识生成

[cite\_start]平台的核心功能由两个独立的 RAG 循环驱动 [cite: 50]。

### 4.1 RAG 循环 A：增强型提示词生成

[cite\_start]此循环是用户与平台的主要交互点 [cite: 52]。

1.  [cite\_start]**用户输入与配置**：用户输入提示词，并选择要激活的子知识库 [cite: 53]。
2.  [cite\_start]**Agentic Tool Selection**：后端根据用户的选择，将每个激活的知识库（Collection 或 KG）视为一个可调用的检索工具 [cite: 54, 10]。
3.  [cite\_start]**多源并行检索**：系统对主知识库和所有选定的子知识库并行执行检索 [cite: 55]。
4.  [cite\_start]**上下文聚合与去噪**：合并所有检索结果 [cite: 56]。
5.  [cite\_start]**Metaprompting**：将聚合后的上下文与原始问题包裹在一个元提示词模板中，指导 LLM 生成最终的增强提示词 [cite: 57]。
6.  [cite\_start]**结果返回**：FastAPI 服务将完整的 Metaprompt 作为结果返回 [cite: 58]。

### 4.2 RAG 循环 B：LLM 自动结构化知识生成

[cite\_start]此循环实现了子知识库的自动化创建 [cite: 60]。

1.  [cite\_start]**触发机制**：用户上传项目后或手动触发 [cite: 61]。
2.  [cite\_start]**LLM 代码分析**：后端调用 LLM 对主知识库代码进行分析 [cite: 62]。
3.  **结构化数据提取**：LLM 执行双重任务：
      - [cite\_start]**任务一 (L2a)**：生成概括性的文本描述 [cite: 64]。
      - [cite\_start]**任务二 (L2b - KG)**：提取关系三元组 [cite: 65]。
4.  [cite\_start]**子知识库持久化**：将任务一和任务二的结果分别存入 L2a 和 L2b 知识库 [cite: 67, 68]。
5.  [cite\_start]**元数据更新**：在数据库中记录新生成的子知识库 [cite: 69]。

## V. 前端战略：低代码平台推荐与集成

[cite\_start]用户明确提出不自行编写前端，需要推荐低代码平台 [cite: 71]。

  - [cite\_start]**推荐平台示例**：Appsmith 或 Retool [cite: 73][cite\_start]。这些平台允许通过拖拽组件和配置 RESTful API 调用来快速构建内部应用界面 [cite: 74]。
  - [cite\_start]**集成策略**：前端完全通过 API 驱动，仅调用 FastAPI 提供的端点 [cite: 76]。
  - [cite\_start]**关键组件要求**：文件上传、API配置表单、列表/复选框（用于选择子知识库）、Markdown/代码高亮组件 [cite: 78, 79, 80, 81]。

## VI. 技术栈总结、部署和学习资源

### 6.1 核心技术栈总结与依赖

| 层级 | 组件 | 作用/功能 | Python 库/技术 |
| :--- | :--- | :--- | :--- |
| **后端 API/服务** | Web 框架 | 高性能 API 服务，原生异步支持 | "FastAPI, Uvicorn" |
| **持久化/元数据** | 关系型数据库 | 存储 KB 注册表、Sub-KB 清单、用户配置 | "PostgreSQL/SQLite, SQLAlchemy" |
| **向量数据库** | 向量存储与检索 | 存储 L1, L2a, L3 的向量化数据 | "Qdrant (Docker), qdrant-client" |
| **RAG 编排层** | 检索链管理，工具调用 | 文档处理、多源检索、Agentic RAG | LlamaIndex / LangChain |
| **嵌入模型** | 文本/代码向量化 | 生成高质量的向量嵌入 | FastEmbed / HuggingFace BGE |
| **知识图谱 (L2b)** | 结构化关系存储 | 存储 LLM 提取的 S-P-O 三元组 | LlamaIndex Graph Store / Neo4j (可选) |
| **前端 UI** | 用户界面与 API 交互 | 低代码设计，KB 管理和 RAG 交互界面 | Appsmith / Retool |

### 6.2 部署策略与未来展望

[cite\_start]项目的推荐部署策略是采用容器化技术 [cite: 97][cite\_start]。建议使用 Docker Compose 文件来统一管理 FastAPI 服务和 Qdrant 数据库服务 [cite: 98, 6][cite\_start]。所有敏感密钥必须通过环境变量或 Secret Manager 存储，绝不能硬编码 [cite: 99]。


